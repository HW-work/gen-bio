Takeaways from VAE tutorial

The generative model is able to capture the dependencies betweed dimensions. 

1. Latent Variable Models: We have a vector of latent variables z in a high-dimensional space Z which we can easily sample according to some probability density function (PDF) p(z) defined over Z.  
Say we have a family of deterministic functions f(z;&theta;), parameterized by a vector &theta; in some space &Theta;, 
where f: Z &times; &Theta; &rightarrow; X. 

    p(X) = &int; p(X|z;&theta;)p(z)dz
    
    P(X|z;&theta;) = N(X|f(z;&theta;), &sigma;<sup>2</sup> * I)
    
    p(z) = N(z|0, I)

2. Variational Autoencoders: z can be drawn from a simple distribution, i.e., N(0,I) I is the identity matrix. The key is "any distribution in d dimensions can be generated by taking a set of d variables that are normally distributed and mapping them through a sufficiently complicated function". 

